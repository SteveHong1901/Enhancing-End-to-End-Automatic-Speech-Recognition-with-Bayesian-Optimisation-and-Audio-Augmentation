{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["vZJ4SL1FeQoy","3_wXHIhCepp0","CO4RWSGzhrN-","PQc9DHr71ArQ"],"gpuType":"L4","authorship_tag":"ABX9TyMFUO9NH4Ob6Kp+3mz5jPsS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install jiwer --quiet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8yqDl0HjdVT","executionInfo":{"status":"ok","timestamp":1734140307965,"user_tz":0,"elapsed":5016,"user":{"displayName":"Samuel Belkadi","userId":"06825886529494182834"}},"outputId":"93b70f92-4faf-4fbf-d057-899bf38729d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/3.1 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import torch, torchaudio, json, os, time\n","import numpy as np\n","\n","from torch import nn\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import Dataset, DataLoader\n","from jiwer import compute_measures, cer"],"metadata":{"id":"RnykaOpJdSkl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sync with google drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tdu-F-x6dUkz","executionInfo":{"status":"ok","timestamp":1734140491784,"user_tz":0,"elapsed":15362,"user":{"displayName":"Samuel Belkadi","userId":"06825886529494182834"}},"outputId":"c7bea68a-e859-44ca-d0e7-d3d6e2848f06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["TRAIN_DATASET_NAME = \"synthetic_train_x3_noise=0.0.json\"\n","TRAIN_DATASET_JSON_DIR = f\"drive/MyDrive/MLMI2/proc_data/augmented_data/json/{TRAIN_DATASET_NAME}\"\n","VALID_DATASET_JSON_DIR = \"drive/MyDrive/MLMI2/proc_data/json/valid.json\"\n","\n","TRAIN_MFCC_DATA_DIR = \"drive/MyDrive/MLMI2/proc_data/augmented_data/synthetic_mfcc.zip\"\n","VALID_MFCC_DATA_DIR = \"drive/MyDrive/MLMI2/proc_data/mfcc_features/mfcc_features.zip\"\n","\n","MODEL_PARAMETERS = {\n","    'batch_size': 8,\n","    'nb_hidden': 3,\n","    'hidden_size': 512,\n","    'model_arch': 'BI-LSTM',\n","    'optimizer': 'SGD_SCHEDULED',\n","    'learning_rate': 0.02,\n","    'momentum': 0.9,\n","    'dropout_rate': 0.2,\n","    'max_norm_clipping': 4.0,\n","    'patience': 7,\n","    'min_reduce': 0.03,\n","    'max_epoch': 55,\n","    'scheduler_patience': 2,\n","}\n","\n","DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","SEED = 123\n","torch.manual_seed(SEED)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3RDJcV3bbwd4","executionInfo":{"status":"ok","timestamp":1734140491784,"user_tz":0,"elapsed":2,"user":{"displayName":"Samuel Belkadi","userId":"06825886529494182834"}},"outputId":"90cf9d11-f32e-4800-d39c-0e1281fe62fd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7dfd65b4dcf0>"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["### Model"],"metadata":{"id":"vZJ4SL1FeQoy"}},{"cell_type":"code","source":["class MyLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, nb_hidden, output_size, is_bidirectional, dropout_rate):\n","        super(MyLSTM, self).__init__()\n","        self.lstm = nn.LSTM(input_size, hidden_size, nb_hidden, dropout=dropout_rate, bidirectional=is_bidirectional)\n","        if is_bidirectional:\n","          self.fc = nn.Linear(hidden_size*2, output_size)\n","        else:\n","          self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        hidden, _ = self.lstm(x)\n","        out = self.fc(hidden)\n","        return out"],"metadata":{"id":"woMKq5s-dMIt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model initialisation\n","model = MyLSTM(\n","    input_size=23,\n","    hidden_size=MODEL_PARAMETERS['hidden_size'],\n","    nb_hidden=MODEL_PARAMETERS['nb_hidden'],\n","    output_size=40,\n","    is_bidirectional=(MODEL_PARAMETERS['model_arch'] == 'BI-LSTM'),\n","    dropout_rate=MODEL_PARAMETERS['dropout_rate'],\n",").to(DEVICE)"],"metadata":{"id":"9R-KdGeAeTdr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dataset"],"metadata":{"id":"3_wXHIhCepp0"}},{"cell_type":"code","source":["# Get token to phone vocabulary (dictionary)\n","with open('drive/MyDrive/MLMI2/proc_data/vocab_39.txt') as f:\n","  vocab = f.read().splitlines()\n","  vocab_idx_to_phn = {k: v for k, v in enumerate(vocab)}"],"metadata":{"id":"rfT0sXOmjVXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUh-xRtWEkGn"},"outputs":[],"source":["class TDataset(Dataset):\n","    def __init__(self, path, specaug=False):\n","        with open(path) as f:\n","            self.data = json.load(f)\n","        self.specaug = specaug\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        data = self.data[index]\n","        mfcc = torch.load(data[\"mfcc_path\"])\n","        targets = data[\"tokens\"]\n","        return mfcc, targets\n","\n","def collate_wrapper(batch):\n","    fbank = pad_sequence([i[0] for i in batch])\n","    input_lengths = torch.tensor([len(i[0]) for i in batch], dtype=torch.long)\n","    targets = [i[1] for i in batch]\n","    return fbank, input_lengths, targets\n","\n","def get_dataloader(path, bs, shuffle, specaug=False):\n","    dataset = TDataset(path, specaug)\n","    return DataLoader(\n","        dataset,\n","        batch_size=bs,\n","        shuffle=shuffle,\n","        collate_fn=collate_wrapper,\n","        pin_memory=True\n","    )"]},{"cell_type":"code","source":["!cp -f {TRAIN_DATASET_JSON_DIR} .\n","!cp -f {VALID_DATASET_JSON_DIR} .\n","\n","!unzip -q -o {TRAIN_MFCC_DATA_DIR} -d .\n","!unzip -q -o {VALID_MFCC_DATA_DIR} -d ."],"metadata":{"id":"2JO5VA5ffXHl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Setup SYNTHETIC batched data\n","train_loader = get_dataloader(TRAIN_DATASET_NAME, MODEL_PARAMETERS['batch_size'], True)\n","valid_loader = get_dataloader('valid.json', MODEL_PARAMETERS['batch_size'], False)"],"metadata":{"id":"_Wytxzm-ewUw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"CO4RWSGzhrN-"}},{"cell_type":"code","source":["# TRIAL HYPERPARAMETERS\n","OPTIMIZER = MODEL_PARAMETERS['optimizer']\n","LEARNING_RATE = MODEL_PARAMETERS['learning_rate']\n","MAX_NORM_CLIPPING = MODEL_PARAMETERS['max_norm_clipping']\n","MOMENTUM = 0.9"],"metadata":{"id":"A7fG57UVjDXU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# EARLY STOPPING\n","all_valid_loss, PATIENCE, MIN_REDUCE = [], MODEL_PARAMETERS['patience'], MODEL_PARAMETERS['min_reduce']\n","MAX_EPOCH = MODEL_PARAMETERS['max_epoch']"],"metadata":{"id":"IUBea7wmjECp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optimizer initialisation\n","criterion = torch.nn.CTCLoss(blank=0, zero_infinity=True)\n","if OPTIMIZER == 'SGD':\n","  optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n","elif OPTIMIZER == 'SGD_SCHEDULED':\n","  optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n","  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=MODEL_PARAMETERS['scheduler_patience'], threshold=MIN_REDUCE)\n","else:\n","  optimizer = torch.optim.Adam(model.parameters(),lr=LEARNING_RATE)"],"metadata":{"id":"dt_yve1VjFAG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- TRAINING ---\n","start_time = time.time()\n","metrics = {'PER': [], 'sub': [], 'dele': [], 'ins': [], 'cor': [], 'TL': []}\n","\n","for epoch in range(MAX_EPOCH):\n","  train_loss_history = []\n","\n","  model.train()\n","  for batch in train_loader:\n","\n","    input, input_lengths, targets = batch\n","    input = input.to(device=DEVICE)\n","    input_lengths = input_lengths.to(device=DEVICE)\n","\n","    targets = [torch.tensor(tokens) for tokens in targets]\n","    target_lengths = torch.tensor([len(target) for target in targets], dtype=torch.long)\n","    targets = pad_sequence(targets, batch_first=True)\n","    targets = targets.to(DEVICE)\n","\n","    optimizer.zero_grad()\n","\n","    y_pred = model(input).log_softmax(2)\n","    loss = criterion(y_pred, targets, input_lengths, target_lengths)\n","    loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=MAX_NORM_CLIPPING)\n","\n","    optimizer.step()\n","    train_loss_history.append(loss.item())\n","\n","  print('TL:', np.mean(train_loss_history))\n","  metrics['TL'].append(np.mean(train_loss_history))\n","\n","  # --- VALIDATION ---\n","  model.eval()\n","  valid_loss_history = []\n","  decoding_stats = [0., 0., 0., 0.]\n","\n","  with torch.no_grad():\n","\n","    for batch in valid_loader:\n","\n","      input, input_lengths, targets = batch\n","      input = input.to(device=DEVICE)\n","      input_lengths = input_lengths.to(device=DEVICE)\n","\n","      targets = [torch.tensor(tokens) for tokens in targets]\n","      target_lengths = torch.tensor([len(target) for target in targets], dtype=torch.long)\n","      targets = pad_sequence(targets, batch_first=True)\n","      targets = targets.to(DEVICE)\n","\n","      y_pred = model(input).log_softmax(2)\n","      loss = criterion(y_pred, targets, input_lengths, target_lengths)\n","      valid_loss_history.append(loss.item())\n","\n","      # Get PER, sub/del/ins/cor metrics\n","      for i in range(len(batch)):\n","        # Get formatted predictions\n","        pred_softmax = y_pred.transpose(1, 0)[i]\n","        pred = list(int(i) for i in torch.argmax(pred_softmax, dim=-1))[:input_lengths[i]]\n","        pred = [pred[0]] + [pred[i] for i in range(1, len(pred)) if pred[i] != pred[i - 1]] # Delete consequitive duplicates\n","        pred = [i for i in pred if i != 0]          # Remove the blank spaces\n","        pred = [vocab_idx_to_phn[i] for i in pred]  # Transform from tokens to phones\n","        pred = ' '.join(pred)\n","        # Get formatted targets\n","        exp = targets[i][:target_lengths[i]]\n","        exp = [int(i) for i in exp]\n","        exp = [vocab_idx_to_phn[i] for i in exp]\n","        exp = ' '.join(exp)\n","        # Record metrics\n","        cur_stats = compute_measures(exp, pred)\n","        decoding_stats[0] += cur_stats[\"substitutions\"]\n","        decoding_stats[1] += cur_stats[\"deletions\"]\n","        decoding_stats[2] += cur_stats[\"insertions\"]\n","        decoding_stats[3] += cur_stats[\"hits\"]\n","\n","  total_words = decoding_stats[0] + decoding_stats[1] + decoding_stats[3]\n","  sub = decoding_stats[0] / total_words * 100\n","  dele = decoding_stats[1] / total_words * 100\n","  ins = decoding_stats[2] / total_words * 100\n","  cor = decoding_stats[3] / total_words * 100\n","  PER = (decoding_stats[0] + decoding_stats[1] + decoding_stats[2]) / total_words * 100\n","\n","  # Save additional metrics\n","  metrics['PER'].append(PER)\n","  metrics['sub'].append(sub)\n","  metrics['dele'].append(dele)\n","  metrics['ins'].append(ins)\n","  metrics['cor'].append(cor)\n","\n","\n","  print('VL:', np.mean(valid_loss_history))\n","  print('Val-PER:', PER)\n","  all_valid_loss.append(np.mean(valid_loss_history))\n","\n","  # Step scheduler on validation loss\n","  if OPTIMIZER == 'SGD_SCHEDULED':\n","    scheduler.step(np.mean(valid_loss_history))\n","\n","  # CHECK: Early Stopping\n","  if len(all_valid_loss) > PATIENCE:\n","    if min(all_valid_loss[:-PATIENCE]) - min(all_valid_loss[-PATIENCE:]) < MIN_REDUCE:\n","      break\n","\n","metrics['VL'] = all_valid_loss\n","metrics['run_time'] = time.time() - start_time\n","metrics['best_epoch'] = int(np.argmin(all_valid_loss))"],"metadata":{"id":"UGwxJ9A7hsXL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save logs"],"metadata":{"id":"PQc9DHr71ArQ"}},{"cell_type":"code","source":["with open('drive/MyDrive/MLMI2/training/synthetic_logs/logs.txt', 'a+') as f:\n","    f.write(f'{TRAIN_DATASET_NAME} :=>: ')\n","    json.dump(metrics, f)\n","    f.write('\\nParameters: ')\n","    json.dump(MODEL_PARAMETERS, f)\n","    f.write('\\n\\n')"],"metadata":{"id":"-1OQXf0x0-Ll"},"execution_count":null,"outputs":[]}]}